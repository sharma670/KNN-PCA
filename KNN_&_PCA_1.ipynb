{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?"
      ],
      "metadata": {
        "id": "A9v1-0wV5_PX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It is a lazy learning method because it does not learn a model during training; instead, it memorizes the training dataset and makes predictions at the time of testing.\n",
        "\n",
        "- How KNN Works:\n",
        "\n",
        "1. Choose a value of K (number of nearest neighbors to consider).\n",
        "\n",
        "2. For a new data point:\n",
        "\n",
        "- Calculate the distance (commonly Euclidean distance) between the new point and all training points.\n",
        "\n",
        "- Select the K nearest neighbors (points with the smallest distances).\n",
        "\n",
        "- Make prediction based on these neighbors.\n",
        "\n",
        "-  KNN in Classification:\n",
        "\n",
        "- Each of the K neighbors \"votes\" for its class.\n",
        "\n",
        "- The class with the majority votes is assigned to the new data point.\n",
        "\n",
        "- Example:\n",
        "- If K = 5 and among 5 neighbors → 3 belong to Class A and 2 belong to Class B, then the new point will be classified as Class A.\n",
        "\n",
        "- KNN in Regression:\n",
        "\n",
        "- Instead of voting, KNN takes the average (or weighted average) of the target values of the K nearest neighbors.\n",
        "\n",
        "- The predicted value is the mean of neighbors’ values.\n",
        "\n",
        "- Example:\n",
        "If K = 3 and the neighbors have values 10, 12, 14, the predicted value = (10 + 12 + 14) / 3 = 12"
      ],
      "metadata": {
        "id": "eeM8V1wg6YCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?"
      ],
      "metadata": {
        "id": "Oa6VvV_-7Ujf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Curse of Dimensionality refers to the problems that occur when data has a very large number of features (dimensions). As the number of dimensions increases, data points become sparse and the concept of \"closeness\" (distance between points) becomes less meaningful.\n",
        "\n",
        "- How it affects KNN:\n",
        "\n",
        "1. Distance becomes less reliable:\n",
        "\n",
        "- In high dimensions, the distance between the nearest and farthest neighbors becomes almost the same.\n",
        "\n",
        "- This makes it hard for KNN to identify truly \"nearest\" neighbors.\n",
        "\n",
        "2. Increased computational cost:\n",
        "\n",
        "- More features = more distance calculations.\n",
        "\n",
        "- Prediction becomes very slow for large datasets.\n",
        "\n",
        "3. Risk of overfitting:\n",
        "\n",
        "- With too many irrelevant features, KNN may give wrong predictions because noise dominates useful signals.\n",
        "\n",
        "4. Need for more data:\n",
        "\n",
        "- In higher dimensions, a huge amount of data is required to cover the feature space properly, otherwise KNN struggles.\n",
        "\n",
        "- Example:\n",
        "\n",
        "- Imagine you have 2D data (Height, Weight). Distances are easy to measure.\n",
        "\n",
        "- If you add 100+ irrelevant features (like random numbers), then every point looks \"far\" from every other point, and KNN fails to find meaningful neighbors."
      ],
      "metadata": {
        "id": "P_Gf7k_i7j3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?"
      ],
      "metadata": {
        "id": "CGuMiQiE8zwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and statistics.\n",
        "\n",
        "- It transforms the original features into a new set of uncorrelated features called principal components.\n",
        "\n",
        "- These principal components are linear combinations of the original features and are ordered such that:\n",
        "\n",
        "- The first component captures the maximum variance in the data.\n",
        "\n",
        "- The second component captures the maximum remaining variance, and so on.\n",
        "\n",
        "\n",
        "| **Aspect**           | **PCA (Dimensionality Reduction)**                                                | **Feature Selection**                                    |\n",
        "| -------------------- | --------------------------------------------------------------------------------- | -------------------------------------------------------- |\n",
        "| **Method**           | Creates new features (principal components) as combinations of original features. | Selects a subset of the original features.               |\n",
        "| **Goal**             | Reduce dimensionality while keeping maximum variance.                             | Keep only the most relevant features.                    |\n",
        "| **Interpretability** | New features are hard to interpret.                                               | Original features are kept, so easy to interpret.        |\n",
        "| **Type**             | Feature transformation technique.                                                 | Feature elimination/selection technique.                 |\n",
        "| **Use case**         | Useful when features are highly correlated.                                       | Useful when only some features contribute to prediction. |\n"
      ],
      "metadata": {
        "id": "pNtag68D842N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?"
      ],
      "metadata": {
        "id": "kKIsiFym90D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Eigenvalues and Eigenvectors (Basics):\n",
        "\n",
        "- Eigenvectors are directions along which data varies the most. They show the orientation of the new feature space (principal components).\n",
        "\n",
        "- Eigenvalues represent the amount of variance captured by each eigenvector (principal component). A higher eigenvalue means that direction captures more information from the data.\n",
        "\n",
        "- Why are they important in PCA?\n",
        "\n",
        "1. Identify Principal Components:\n",
        "\n",
        "- Eigenvectors define the principal components (new transformed features).\n",
        "\n",
        "- For example, PC1 = eigenvector with the largest eigenvalue.\n",
        "\n",
        "2. Rank Components by Importance:\n",
        "\n",
        "- Eigenvalues tell how much variance each component explains.\n",
        "\n",
        "- We can select only the top k components with the highest eigenvalues to reduce dimensionality.\n",
        "\n",
        "3. Data Compression:\n",
        "\n",
        "- By keeping only components with large eigenvalues, we reduce features while retaining most of the useful information.\n",
        "\n",
        "- Noise Reduction:\n",
        "\n",
        "- Components with very small eigenvalues contribute little variance (often noise), so they can be discarded."
      ],
      "metadata": {
        "id": "5hNoeR9F94tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?"
      ],
      "metadata": {
        "id": "pi_k2W0r-aqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) are often used together in machine learning pipelines because they solve each other’s weaknesses.\n",
        "\n",
        "- How they complement each other:\n",
        "\n",
        "1. PCA reduces dimensionality → improves KNN performance:\n",
        "\n",
        "- KNN suffers from the curse of dimensionality when data has many features.\n",
        "\n",
        "- PCA reduces the number of dimensions while keeping most important information, making distance calculations in KNN more meaningful.\n",
        "\n",
        "2. Noise reduction before KNN:\n",
        "\n",
        "- PCA removes less informative features (small eigenvalues).\n",
        "\n",
        "- This helps KNN focus only on relevant patterns and avoid noisy distances.\n",
        "\n",
        "3. Speed improvement:\n",
        "\n",
        "- KNN requires computing distances from the test point to all training points.\n",
        "\n",
        "- With fewer dimensions after PCA, these distance calculations become faster.\n",
        "\n",
        "4. Better generalization:\n",
        "\n",
        "- PCA prevents overfitting by removing redundant features.\n",
        "\n",
        "- KNN then makes predictions based on a cleaner, compressed representation of the data.\n",
        "\n",
        "- Example Pipeline:\n",
        "\n",
        "1. Start with a dataset having 100 features.\n",
        "\n",
        "2. Apply PCA → reduce to 10 principal components (retaining 95% variance).\n",
        "\n",
        "3. Use KNN on these 10 components → predictions are faster, more accurate, and less noisy."
      ],
      "metadata": {
        "id": "xBlZ6TmG-g0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "6IXxKfpABpsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling   :\", acc_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4JnUBcaCoPs",
        "outputId": "70010a3e-adc6-4d04-c49c-fdd5495e697d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.7222222222222222\n",
            "Accuracy with Scaling   : 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "sTW3YHtTC6u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teqHpgcKDQaF",
        "outputId": "6fd0df65-d787-48c7-abaa-480bcfad105f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "N1AkU4k_EDfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on Original Dataset (13 features):\", acc_original)\n",
        "print(\"Accuracy on PCA Dataset (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT7oVfmMEFqB",
        "outputId": "8bb0b58c-df35-4e7f-8cbc-6434df336804"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Dataset (13 features): 0.9444444444444444\n",
            "Accuracy on PCA Dataset (2 components): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "St4NBkprEzTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "\n",
        "print(\"Accuracy with Euclidean Distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7jUvBXWFH4V",
        "outputId": "4b006454-1edd-4e0c-b663-00492cae50a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9444444444444444\n",
            "Accuracy with Manhattan Distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "rBTzNZ7BFlm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In biomedical datasets (like gene expression), there are often thousands of features (genes) but only a few hundred samples (patients). This creates a high risk of overfitting.\n",
        "To solve this, we use PCA + KNN pipeline.\n",
        "\n",
        "Step 1: Use PCA to Reduce Dimensionality\n",
        "\n",
        "- PCA transforms the high-dimensional gene data into a smaller set of principal components.\n",
        "\n",
        "- This reduces noise and correlation between features while retaining most of the variance (information).\n",
        "\n",
        "Step 2: Decide How Many Components to Keep\n",
        "\n",
        "- We look at the explained variance ratio (scree plot).\n",
        "\n",
        "- We choose the smallest number of components that capture ~90–95% variance.\n",
        "\n",
        "- This ensures a balance between information retention and model simplicity.\n",
        "\n",
        "Step 3: Use KNN for Classification Post-PCA\n",
        "\n",
        "- After dimensionality reduction, apply KNN.\n",
        "\n",
        "- KNN works better in lower dimensions since distance metrics are more reliable.\n",
        "\n",
        "Step 4: Evaluate the Model\n",
        "\n",
        "- Split dataset into train/test.\n",
        "\n",
        "- Compute accuracy score (and optionally confusion matrix or F1-score).\n",
        "\n",
        "- Compare performance with and without PCA.\n",
        "\n",
        "Step 5: Justify to Stakeholders\n",
        "\n",
        "- Biomedical data is noisy & high-dimensional → PCA removes irrelevant variation.\n",
        "\n",
        "- PCA ensures interpretability (components reflect major patterns in gene activity).\n",
        "\n",
        "- KNN is simple, transparent, and non-parametric → doctors/researchers can trust predictions.\n",
        "\n",
        "- This pipeline provides a robust, generalizable solution without overfitting."
      ],
      "metadata": {
        "id": "Qq8YlF5wF9Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original features:\", X.shape[1])\n",
        "print(\"Reduced PCA components:\", X_train_pca.shape[1])\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy after PCA + KNN:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKiBuOrnGbHw",
        "outputId": "9c6db51e-fe47-4fc3-92aa-e01fe1d00116"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 13\n",
            "Reduced PCA components: 10\n",
            "Accuracy after PCA + KNN: 0.9444444444444444\n"
          ]
        }
      ]
    }
  ]
}